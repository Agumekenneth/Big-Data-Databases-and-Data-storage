{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b069023b",
   "metadata": {},
   "source": [
    "# AGUME KENNETH B30309 S24B38/017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed0dd32",
   "metadata": {},
   "source": [
    "## SECTION A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6e0c3",
   "metadata": {},
   "source": [
    "## Q1. CSV vs Parquet (Core)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94028d00",
   "metadata": {},
   "source": [
    " How CSV stores data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494365d",
   "metadata": {},
   "source": [
    "- In my experiment, the CSV file size was 32.86 MB while the Parquet file was only 7.03 MB. CSV stores data in plain text format, where each row is written sequentially and values are separated by commas. This means numbers and categories are stored as readable text which increases file size and requires more disk reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7683b",
   "metadata": {},
   "source": [
    "How Parquet stores data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afdcfd",
   "metadata": {},
   "source": [
    "- Parquet, on the other hand, stores data in a columnar format. Each column is stored separately and compressed efficiently. This is because similar data types are stored together thus making compression  more effective. This explains why the Parquet file was significantly smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06be1a8",
   "metadata": {},
   "source": [
    "What this means for disk I/O?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb8ea4",
   "metadata": {},
   "source": [
    "- The CSV loaded slightly fastee ir 1.09s vs 1.51s, Parquet reduces disk I/O overall because less data needs to be physically read from disk. As data grows larger, reduced disk I/O becomes more important than small decompression overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c2ba30",
   "metadata": {},
   "source": [
    "## Q2. Column Selection Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6f9e7",
   "metadata": {},
   "source": [
    "Why is this possible with Parquet but not efficient with CSV?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43133e66",
   "metadata": {},
   "source": [
    "- When I loaded only two columns from the Parquet file, the load time dropped to 0.09 seconds because Parquet stores data column-by-column thus allowing the system to read only the required columns from disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c32b9b",
   "metadata": {},
   "source": [
    "Relate your answer to columnar storage design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c04672",
   "metadata": {},
   "source": [
    "- CSV files store data row-by-row even when only two columns are needed, the entire row must be read and parsed thus making selective column reading inefficient in CSV. My experiment clearly demonstrated that columnar storage improves performance when working with analytical queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a69bdbd",
   "metadata": {},
   "source": [
    "## Q3. Storage as a Bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b1ce5",
   "metadata": {},
   "source": [
    "- Loading the full dataset increased memory usage from 90 MB to 127 MB. Even though the dataset was only 32.86 MB on disk, reading it required additional memory and parsing time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6dd180",
   "metadata": {},
   "source": [
    "- As datasets increase in size, disk reading becomes a major bottleneck because data must first be transferred from storage into memory before processing. Disk I/O is slower than CPU operations therefore, even before CPU becomes overloaded, the speed of reading data from disk limits performance. This is why Big Data systems optimize storage formats and reduce unnecessary reads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171bb4d7",
   "metadata": {},
   "source": [
    "## SECTION B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37501d25",
   "metadata": {},
   "source": [
    "## Q4. Full Load vs Chunk Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3284c",
   "metadata": {},
   "source": [
    "- Loading the entire CSV file at once increased memory usage and required the system to hold all rows in RAM simultaneously.\n",
    "- While this worked for 500,000 rows, it would not scale well for millions or billions of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668ec41",
   "metadata": {},
   "source": [
    "- Chunk processing worked more reliably because only 50,000 rows were loaded at a time. After processing each chunk, the memory was freed before loading the next one. This reduced memory pressure and made the process more stable thus preventing crashes and improving scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d9df7",
   "metadata": {},
   "source": [
    "## Q5. Chunk Processing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a68bf",
   "metadata": {},
   "source": [
    "What a chunk represents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d99abf",
   "metadata": {},
   "source": [
    "- A chunk represents a small portion of the dataset, in my case 50,000 rows. Instead of loading all rows into memory, the file was divided into manageable blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d9476",
   "metadata": {},
   "source": [
    "How partial results were combined?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e56c6e",
   "metadata": {},
   "source": [
    "- For each chunk, partial results were computed. For example, when calculating average transaction value per category, I accumulated partial sums and counts for each category. After all chunks were processed, the final averages were calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5339f04",
   "metadata": {},
   "source": [
    "Why this approach scales better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a0581",
   "metadata": {},
   "source": [
    "- This approach scales better because memory usage remains constant regardless of dataset size. Only a fixed-size chunk is processed at any time, making it suitable for very large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1321da",
   "metadata": {},
   "source": [
    "## Q6. Manual Effort Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024c50d",
   "metadata": {},
   "source": [
    "- Chunk processing required manually maintaining dictionaries to accumulate partial sums and counts. It also required explicit loops and aggregation logic across chunks which made the process more complex compared to using simple pandas groupby on a fully loaded DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93a856f",
   "metadata": {},
   "source": [
    "- Big Data systems automate this process by distributing data automatically and combining results behind the scenes. For example, systems like Hadoop and Spark handle partitioning and aggregation without manual tracking. This reduces coding complexity and human error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7e584",
   "metadata": {},
   "source": [
    "## SECTION C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab51b9f2",
   "metadata": {},
   "source": [
    "## Q7. From Chunks to Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f896e7",
   "metadata": {},
   "source": [
    "- The chunk processing i implemented is conceptually similar to data partitioning in Big data systems. Each chunk represents a small partition of the overall dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f80dfd",
   "metadata": {},
   "source": [
    "- In distributed systems, data is split across multiple machines instead of being read sequentially on one machine. Each machine processes its partition independently and the results are combined. My chunk logic mimics this idea, but sequentially on a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b709300",
   "metadata": {},
   "source": [
    "## Q8. Why Distributed Storage Is Necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42bb360",
   "metadata": {},
   "source": [
    "- If the dataset grew to several terabytes, storing it on a single machine would be impractical. First, storage capacity would be insufficient. Second, reading and writing such large files would be extremely slow on one disk. Third, a single machine represents a single point of failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3308c",
   "metadata": {},
   "source": [
    "- Distributed storage systems solve these problems by spreading data across multiple machines. This increases storage capacity, improves read or write performance through parallelism, and ensures fault tolerance through replication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0978334",
   "metadata": {},
   "source": [
    "## Q9. Moving Computation to the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f365d18",
   "metadata": {},
   "source": [
    "- During the first assignment, all processing occurred on my local machine. Even with 500,000 rows, loading data into memory increased RAM usage noticeably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882edc7b",
   "metadata": {},
   "source": [
    "- If the dataset were terabytes in size, transferring all data to a single processor would be inefficient and slow. Big Data systems move computation to where data is stored to reduce network traffic and avoid transferring massive datasets. This improves performance and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21bad88",
   "metadata": {},
   "source": [
    "## SECTION D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96960bc5",
   "metadata": {},
   "source": [
    "## Q10. Hadoop Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8460a",
   "metadata": {},
   "source": [
    "- Hadoop was created to address problems such as large file sizes and memory constraints. In my assignment, I observed that loading large files increases memory usage and requires careful chunk processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abbe2cd",
   "metadata": {},
   "source": [
    "- Hadoop solves this by splitting files into blocks and distributing them across multiple machines. It also provides fault tolerance through data replication. This removes the limitations of sequential processing on a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7428c76d",
   "metadata": {},
   "source": [
    "## Q11. Why Spark Improves on Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4040c79e",
   "metadata": {},
   "source": [
    "- In my assignment, repeated file reads were required during experimentation. Each time the file was reloaded, it took additional time and memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b830dea6",
   "metadata": {},
   "source": [
    "- Spark improves on Hadoop by keeping data in memory across operations. This reduces repeated disk reads and significantly speeds up iterative or interactive analytics. Spark is therefore better suited for data exploration and machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b067a8",
   "metadata": {},
   "source": [
    "## SECTION E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c458162",
   "metadata": {},
   "source": [
    "## Q12. Limits of Single-Machine Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350614d7",
   "metadata": {},
   "source": [
    "- Using a more powerful computer may temporarily improve performance but it does not solve scalability problems. Hardware upgrades have limits and become expensive. Memory and disk capacity remain finite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba232b",
   "metadata": {},
   "source": [
    "- Big Data problems require horizontal scaling across multiple machines rather than vertical scaling on one powerful machine. My experiment already showed increasing memory usage with moderate data size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb2f4d5",
   "metadata": {},
   "source": [
    "## Q13. Conceptual Architecture Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3165073",
   "metadata": {},
   "source": [
    "In a modern Big Data system:\n",
    "- Storage (e.g HDFS or cloud storage) stores large datasets.\n",
    "- File format (e.g Parquet) determines how data is physically organized.\n",
    "- Computation (e.g Spark) processes data where it is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f0359",
   "metadata": {},
   "source": [
    "- In my assignment, the above components were combined on a single machine. However, Big Data systems separate them to improve scalability and flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf8dab",
   "metadata": {},
   "source": [
    "## Q14. Fault Tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440fce43",
   "metadata": {},
   "source": [
    "- My laptop was a single point of failure. If it crashed, all processing would stop. Distributed storage systems replicate data across multiple machines. If one machine fails, another replica is used. This ensures reliability and continuous operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e327c",
   "metadata": {},
   "source": [
    "## Q15. Real-World Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f080e04",
   "metadata": {},
   "source": [
    "- If this approach were applied to national telecom logs, the data would quickly reach terabytes or petabytes. Chunk-based processing on a single laptop would be too slow and unreliable. Distributed storage (HDFS or cloud storage), Spark for processing, and possibly Kafka for streaming would become necessary to handle real-time and large-scale analytics."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
