{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b069023b",
   "metadata": {},
   "source": [
    "# AGUME KENNETH B30309 S24B38/017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed0dd32",
   "metadata": {},
   "source": [
    "## SECTION A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6e0c3",
   "metadata": {},
   "source": [
    "## Q1. CSV vs Parquet (Core)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94028d00",
   "metadata": {},
   "source": [
    " How CSV stores data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494365d",
   "metadata": {},
   "source": [
    "- In my experiment, the CSV file size was 32.86 MB while the Parquet file was only 7.03 MB. CSV stores data in plain text format, where each row is written sequentially and values are separated by commas. This means numbers and categories are stored as readable text which increases file size and requires more disk reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7683b",
   "metadata": {},
   "source": [
    "How Parquet stores data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afdcfd",
   "metadata": {},
   "source": [
    "- Parquet, on the other hand, stores data in a columnar format. Each column is stored separately and compressed efficiently. This is because similar data types are stored together thus making compression  more effective. This explains why the Parquet file was significantly smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06be1a8",
   "metadata": {},
   "source": [
    "What this means for disk I/O?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb8ea4",
   "metadata": {},
   "source": [
    "- The CSV loaded slightly fastee ir 1.09s vs 1.51s, Parquet reduces disk I/O overall because less data needs to be physically read from disk. As data grows larger, reduced disk I/O becomes more important than small decompression overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c2ba30",
   "metadata": {},
   "source": [
    "## Q2. Column Selection Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6f9e7",
   "metadata": {},
   "source": [
    "Why is this possible with Parquet but not efficient with CSV?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43133e66",
   "metadata": {},
   "source": [
    "- When I loaded only two columns from the Parquet file, the load time dropped to 0.09 seconds because Parquet stores data column-by-column thus allowing the system to read only the required columns from disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c32b9b",
   "metadata": {},
   "source": [
    "Relate your answer to columnar storage design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c04672",
   "metadata": {},
   "source": [
    "- CSV files store data row-by-row even when only two columns are needed, the entire row must be read and parsed thus making selective column reading inefficient in CSV. My experiment clearly demonstrated that columnar storage improves performance when working with analytical queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a69bdbd",
   "metadata": {},
   "source": [
    "## Q3. Storage as a Bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b1ce5",
   "metadata": {},
   "source": [
    "- Loading the full dataset increased memory usage from 90 MB to 127 MB. Even though the dataset was only 32.86 MB on disk, reading it required additional memory and parsing time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6dd180",
   "metadata": {},
   "source": [
    "- As datasets increase in size, disk reading becomes a major bottleneck because data must first be transferred from storage into memory before processing. Disk I/O is slower than CPU operations therefore, even before CPU becomes overloaded, the speed of reading data from disk limits performance. This is why Big Data systems optimize storage formats and reduce unnecessary reads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171bb4d7",
   "metadata": {},
   "source": [
    "## SECTION B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37501d25",
   "metadata": {},
   "source": [
    "## Q4. Full Load vs Chunk Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3284c",
   "metadata": {},
   "source": [
    "- Loading the entire CSV file at once increased memory usage and required the system to hold all rows in RAM simultaneously.\n",
    "- While this worked for 500,000 rows, it would not scale well for millions or billions of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668ec41",
   "metadata": {},
   "source": [
    "- Chunk processing worked more reliably because only 50,000 rows were loaded at a time. After processing each chunk, the memory was freed before loading the next one. This reduced memory pressure and made the process more stable thus preventing crashes and improving scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d9df7",
   "metadata": {},
   "source": [
    "## Q5. Chunk Processing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a68bf",
   "metadata": {},
   "source": [
    "What a chunk represents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d99abf",
   "metadata": {},
   "source": [
    "- A chunk represents a small portion of the dataset, in my case 50,000 rows. Instead of loading all rows into memory, the file was divided into manageable blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d9476",
   "metadata": {},
   "source": [
    "How partial results were combined?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e56c6e",
   "metadata": {},
   "source": [
    "- For each chunk, partial results were computed. For example, when calculating average transaction value per category, I accumulated partial sums and counts for each category. After all chunks were processed, the final averages were calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5339f04",
   "metadata": {},
   "source": [
    "Why this approach scales better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a0581",
   "metadata": {},
   "source": [
    "- This approach scales better because memory usage remains constant regardless of dataset size. Only a fixed-size chunk is processed at any time, making it suitable for very large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1321da",
   "metadata": {},
   "source": [
    "## Q6. Manual Effort Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024c50d",
   "metadata": {},
   "source": [
    "- Chunk processing required manually maintaining dictionaries to accumulate partial sums and counts. It also required explicit loops and aggregation logic across chunks which made the process more complex compared to using simple pandas groupby on a fully loaded DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93a856f",
   "metadata": {},
   "source": [
    "- Big Data systems automate this process by distributing data automatically and combining results behind the scenes. For example, systems like Hadoop and Spark handle partitioning and aggregation without manual tracking. This reduces coding complexity and human error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7e584",
   "metadata": {},
   "source": [
    "## SECTION C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab51b9f2",
   "metadata": {},
   "source": [
    "## Q7. From Chunks to Partitions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
