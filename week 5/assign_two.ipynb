{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b069023b",
   "metadata": {},
   "source": [
    "# AGUME KENNETH B30309 S24B38/017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed0dd32",
   "metadata": {},
   "source": [
    "## SECTION A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6e0c3",
   "metadata": {},
   "source": [
    "## Q1. CSV vs Parquet (Core)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94028d00",
   "metadata": {},
   "source": [
    " How CSV stores data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494365d",
   "metadata": {},
   "source": [
    "- In my experiment, the CSV file size was 32.86 MB while the Parquet file was only 7.03 MB. CSV stores data in plain text format, where each row is written sequentially and values are separated by commas. This means numbers and categories are stored as readable text which increases file size and requires more disk reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7683b",
   "metadata": {},
   "source": [
    "How Parquet stores data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afdcfd",
   "metadata": {},
   "source": [
    "- Parquet, on the other hand, stores data in a columnar format. Each column is stored separately and compressed efficiently. This is because similar data types are stored together thus making compression  more effective. This explains why the Parquet file was significantly smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06be1a8",
   "metadata": {},
   "source": [
    "What this means for disk I/O?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb8ea4",
   "metadata": {},
   "source": [
    "- The CSV loaded slightly fastee ir 1.09s vs 1.51s, Parquet reduces disk I/O overall because less data needs to be physically read from disk. As data grows larger, reduced disk I/O becomes more important than small decompression overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c2ba30",
   "metadata": {},
   "source": [
    "## Q2. Column Selection Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6f9e7",
   "metadata": {},
   "source": [
    "Why is this possible with Parquet but not efficient with CSV?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43133e66",
   "metadata": {},
   "source": [
    "- When I loaded only two columns from the Parquet file, the load time dropped to 0.09 seconds because Parquet stores data column-by-column thus allowing the system to read only the required columns from disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c32b9b",
   "metadata": {},
   "source": [
    "Relate your answer to columnar storage design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c04672",
   "metadata": {},
   "source": [
    "- CSV files store data row-by-row even when only two columns are needed, the entire row must be read and parsed thus making selective column reading inefficient in CSV. My experiment clearly demonstrated that columnar storage improves performance when working with analytical queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a69bdbd",
   "metadata": {},
   "source": [
    "## Q3. Storage as a Bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b1ce5",
   "metadata": {},
   "source": [
    "- Loading the full dataset increased memory usage from 90 MB to 127 MB. Even though the dataset was only 32.86 MB on disk, reading it required additional memory and parsing time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6dd180",
   "metadata": {},
   "source": [
    "- As datasets increase in size, disk reading becomes a major bottleneck because data must first be transferred from storage into memory before processing. Disk I/O is slower than CPU operations therefore, even before CPU becomes overloaded, the speed of reading data from disk limits performance. This is why Big Data systems optimize storage formats and reduce unnecessary reads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171bb4d7",
   "metadata": {},
   "source": [
    "## SECTION B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37501d25",
   "metadata": {},
   "source": [
    "## Q4. Full Load vs Chunk Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3284c",
   "metadata": {},
   "source": [
    "- Loading the entire CSV file at once increased memory usage and required the system to hold all rows in RAM simultaneously.\n",
    "- While this worked for 500,000 rows, it would not scale well for millions or billions of rows."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
