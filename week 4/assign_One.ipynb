{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ebc605c",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302fea75",
   "metadata": {},
   "source": [
    "- pandas  for working with tables (DataFrames)\n",
    "- numpy  for generating random data efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3feeaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef972190",
   "metadata": {},
   "source": [
    "- Generating the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00399ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500_000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c71432a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a seed so that results are reproducible\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990ce0c",
   "metadata": {},
   "source": [
    "# Creating the dataset using a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f62e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "\n",
    "    # Unique ID for each transaction\n",
    "    \"transaction_id\": np.arange(1, N + 1),\n",
    "\n",
    "    # Generate random dates within one year\n",
    "    \"date\": pd.to_datetime(\"2023-01-01\") + pd.to_timedelta(\n",
    "        np.random.randint(0, 365, N), unit=\"D\"\n",
    "    ),\n",
    "\n",
    "    # Randomly assign regions\n",
    "    \"region\": np.random.choice(\n",
    "        [\"Central\", \"Eastern\", \"Western\", \"Northern\"], N\n",
    "    ),\n",
    "\n",
    "    # Random product categories\n",
    "    \"product_category\": np.random.choice(\n",
    "        [\"Food\", \"Electronics\", \"Clothing\", \"Furniture\"], N\n",
    "    ),\n",
    "\n",
    "    # Quantity of items bought (1 to 9)\n",
    "    \"quantity\": np.random.randint(1, 10, N),\n",
    "\n",
    "    # Unit price between 5 and 500\n",
    "    \"unit_price\": np.round(np.random.uniform(5, 500, N), 2),\n",
    "\n",
    "    # Payment methods\n",
    "    \"payment_method\": np.random.choice(\n",
    "        [\"Cash\", \"Mobile Money\", \"Card\"], N\n",
    "    ),\n",
    "\n",
    "    # Customer type\n",
    "    \"customer_type\": np.random.choice(\n",
    "        [\"Retail\", \"Wholesale\"], N\n",
    "    )\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e0591",
   "metadata": {},
   "source": [
    "- Creating a derived column for total transaction amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1bffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (23.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.12/bin/python3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acfb317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created successfully!\n"
     ]
    }
   ],
   "source": [
    "# total_amount = quantity * unit_price\n",
    "data[\"total_amount\"] = data[\"quantity\"] * data[\"unit_price\"]\n",
    "\n",
    "# Save the dataset as a CSV file (row-based storage)\n",
    "data.to_csv(\"transactions.csv\", index=False)\n",
    "\n",
    "# Save the dataset as a Parquet file (column-based storage)\n",
    "data.to_parquet(\"transactions.parquet\")\n",
    "\n",
    "print(\"Dataset created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ae0cc",
   "metadata": {},
   "source": [
    "# File size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f5ef31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file size: 32.86 MB\n",
      "Parquet file size: 7.03 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Get file size of CSV in megabytes\n",
    "csv_size_mb = os.path.getsize(\"transactions.csv\") / (1024 * 1024)\n",
    "\n",
    "# Get file size of Parquet in megabytes\n",
    "parquet_size_mb = os.path.getsize(\"transactions.parquet\") / (1024 * 1024)\n",
    "\n",
    "print(f\"CSV file size: {csv_size_mb:.2f} MB\")\n",
    "print(f\"Parquet file size: {parquet_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5f0b1",
   "metadata": {},
   "source": [
    "- The Parquet file is smaller than the CSV file because Parquet uses column-based storage and compression. CSV stores data as plain text, which is inefficient for large datasets. Big Data systems prefer columnar formats like Parquet because they reduce disk usage and improve query performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c14f6",
   "metadata": {},
   "source": [
    "# Read performance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33cf86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ba304a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV load time: 1.09 seconds\n"
     ]
    }
   ],
   "source": [
    "# Loading the entire CSV file\n",
    "start_time = time.time()\n",
    "df_csv = pd.read_csv(\"transactions.csv\")\n",
    "csv_load_time = time.time() - start_time\n",
    "print(f\"CSV load time: {csv_load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c998163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet load time: 1.51 seconds\n"
     ]
    }
   ],
   "source": [
    "#Loading the entire Parquet file\n",
    "start_time = time.time()\n",
    "df_parquet = pd.read_parquet(\"transactions.parquet\")\n",
    "parquet_load_time = time.time() - start_time\n",
    "print(f\"Parquet load time: {parquet_load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "addfcc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet (2 columns) load time: 0.09 seconds\n"
     ]
    }
   ],
   "source": [
    "#Loading only selected columns from Parquet\n",
    "start_time = time.time()\n",
    "df_parquet_cols = pd.read_parquet(\n",
    "    \"transactions.parquet\",\n",
    "    columns=[\"region\", \"total_amount\"]\n",
    ")\n",
    "parquet_cols_load_time = time.time() - start_time\n",
    "\n",
    "print(f\"Parquet (2 columns) load time: {parquet_cols_load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af53395",
   "metadata": {},
   "source": [
    "- Loading the Parquet file is faster than loading the CSV file because Parquet is optimized for analytical workloads. When only specific columns are selected, Parquet loads even faster and uses less memory. This demonstrates why Big Data systems avoid reading unnecessary data and instead scan only required columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da1fc80",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f49d36",
   "metadata": {},
   "source": [
    "- The CSV file size is: 32.86 MB\n",
    "- The Parquet file size is: 7.03 MB\n",
    "- The CSV file load time is: 1.09 seconds\n",
    "- The Parquet file load time: 1.51 seconds\n",
    "- The Parquet (2 columns) file load time is: 0.09 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c9464",
   "metadata": {},
   "source": [
    "The Parquet file was significantly smaller than the CSV file (7.03 MB vs 32.86 MB). This is because Parquet uses column-based storage and compression, while CSV stores data as plain text. Big Data systems prefer Parquet to reduce storage costs and improve I/O efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca44e9",
   "metadata": {},
   "source": [
    "Although the CSV file loaded slightly faster than the Parquet file, this is expected for moderate-sized datasets due to Parquetâ€™s decompression overhead. However, when only two columns were selected, Parquet loaded significantly faster. This demonstrates the advantage of columnar storage in analytical workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31513379",
   "metadata": {},
   "source": [
    "# Memory Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676998fd",
   "metadata": {},
   "source": [
    "- The memory used before was 90MB's loading the parquet file thus moderate\n",
    "- The memory used after by the RAM was 127MB's thus high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468fe4e2",
   "metadata": {},
   "source": [
    "- Memory usage increased from approximately 90 MB to 127 MB when loading the full dataset, indicating higher RAM consumption during full data scans. Column selection from Parquet reduced memory usage, highlighting why Big Data systems avoid reading unnecessary columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be061ac",
   "metadata": {},
   "source": [
    "# Big data problem on a small machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40315f",
   "metadata": {},
   "source": [
    "- Defining the number of rows to read at a time this is aimed at reducing memory usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce890e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 50_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a08428",
   "metadata": {},
   "source": [
    "- Creating the total number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02b69285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records: 500000\n"
     ]
    }
   ],
   "source": [
    "# Initializing a counter for total records\n",
    "total_records = 0\n",
    "\n",
    "# Reading the CSV file in chunks\n",
    "for chunk in pd.read_csv(\"transactions.csv\", chunksize=CHUNK_SIZE):\n",
    "    \n",
    "    # Adding the number of rows in the current chunk\n",
    "    total_records += len(chunk)\n",
    "print(\"Total number of records:\", total_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc4a65",
   "metadata": {},
   "source": [
    "- The total number of records was calculated by reading the CSV file in chunks instead of loading the entire dataset into memory. This approach prevents memory overload and is suitable for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f2c633",
   "metadata": {},
   "source": [
    "# Average Transaction Value per Product category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e477b456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average transaction value per category:\n",
      "{'Clothing': 1259.08098429613, 'Electronics': 1262.440670266557, 'Food': 1260.870070498439, 'Furniture': 1260.529748216433}\n"
     ]
    }
   ],
   "source": [
    "# Dictionaries to store cumulative sums and counts\n",
    "total_amount_sum = {}\n",
    "transaction_count = {}\n",
    "\n",
    "# Process the CSV file chunk by chunk\n",
    "for chunk in pd.read_csv(\"transactions.csv\", chunksize=CHUNK_SIZE):\n",
    "    \n",
    "    # Group data by product category within the chunk\n",
    "    grouped = chunk.groupby(\"product_category\")[\"total_amount\"].agg([\"sum\", \"count\"])\n",
    "    \n",
    "    # Accumulate results across all chunks\n",
    "    for category, row in grouped.iterrows():\n",
    "        total_amount_sum[category] = total_amount_sum.get(category, 0) + row[\"sum\"]\n",
    "        transaction_count[category] = transaction_count.get(category, 0) + row[\"count\"]\n",
    "\n",
    "# Calculate the average transaction value per category\n",
    "average_transaction_value = {\n",
    "    category: total_amount_sum[category] / transaction_count[category]\n",
    "    for category in total_amount_sum\n",
    "}\n",
    "\n",
    "print(\"Average transaction value per category:\")\n",
    "print(average_transaction_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e2298",
   "metadata": {},
   "source": [
    "- The average transaction value per category was computed by aggregating sums and counts incrementally across chunks. This avoids loading the entire dataset into memory while still producing accurate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a192c",
   "metadata": {},
   "source": [
    "# Top 5 regions by Total Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "731285b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 regions by total sales:\n",
      "[('Western', 157864758.64999998), ('Northern', 157520575.82999998), ('Central', 157511204.47), ('Eastern', 157469367.06000003)]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary for storing the total sales per region\n",
    "region_total_sales = {}\n",
    "\n",
    "# Reading the CSV file in chunks\n",
    "for chunk in pd.read_csv(\"transactions.csv\", chunksize=CHUNK_SIZE):\n",
    "    \n",
    "    # Calculating total sales per region in the chunk\n",
    "    grouped = chunk.groupby(\"region\")[\"total_amount\"].sum()\n",
    "    \n",
    "    # Accumulating totals across all chunks\n",
    "    for region, amount in grouped.items():\n",
    "        region_total_sales[region] = region_total_sales.get(region, 0) + amount\n",
    "\n",
    "# Sorting the regions by total sales (descending) and select top 5\n",
    "top_5_regions = sorted(\n",
    "    region_total_sales.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:5]\n",
    "\n",
    "print(\"Top 5 regions by total sales:\")\n",
    "print(top_5_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb49cf7",
   "metadata": {},
   "source": [
    "# Why Chunking is necessary?\n",
    "- Chunking is necessary because large datasets may not fit entirely into memory. By reading small portions of data at a time, memory usage is controlled and the program remains stable.\n",
    "\n",
    "# How This Maps to Hadoop / Big Data Systems.\n",
    "- This approach is similar to how Hadoop processes data by splitting files into blocks and distributing them across nodes. Each node processes a portion of the data, and the partial results are combined to produce the final output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f18541",
   "metadata": {},
   "source": [
    "# Python vs R reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed939f",
   "metadata": {},
   "source": [
    "# How would this task typically be handled in R?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec09d4",
   "metadata": {},
   "source": [
    "- In R, this task is usually handled by loading the dataset into memory using data frames. Packages such as dplyr and data.table are commonly used for data manipulation. For moderately large datasets, fread() from data.table can improve performance. However, R is largely designed for in-memory analysis. When datasets grow very large, memory limitations become a challenge. Extra tools or database connections are required to scale. This makes R less suitable for Big Data without additional infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14b2d8",
   "metadata": {},
   "source": [
    "# What breaks down when datasets become very large?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22046f7",
   "metadata": {},
   "source": [
    "- When datasets become very large, memory becomes the main limitation. Loading the entire dataset into RAM may cause the system to slow down or crash. Operations such as grouping and sorting become computationally expensive. Disk input and output also become a bottleneck. Traditional single-machine tools struggle to scale efficiently. This is why Big Data frameworks are required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0104801",
   "metadata": {},
   "source": [
    "# Why does Big Data move computation to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a122a",
   "metadata": {},
   "source": [
    "- Moving large datasets across networks is slow and costly. Big Data systems instead send computation to where the data is stored. This reduces network traffic and improves performance. It also allows parallel processing across multiple machines. By processing data locally, systems achieve better scalability and fault tolerance. This design principle is central to Big Data architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb08f18",
   "metadata": {},
   "source": [
    "# Storage Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329e0da",
   "metadata": {},
   "source": [
    "- If a dataset grows to 5 TB,\n",
    "\n",
    "# Where would it be stored?\n",
    "- If the dataset grew to 5 TB, it would be stored in a distributed storage system such as Hadoop Distributed File System (HDFS) or cloud object storage like Amazon S3 or Google Cloud Storage. These systems are designed to handle large volumes of data reliably"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0c944e",
   "metadata": {},
   "source": [
    "# How would it be partitioned?\n",
    "- The data would be partitioned based on logical fields such as date or region. Partitioning improves query performance by allowing systems to scan only relevant subsets of data. It also enables parallel processing across multiple nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d770a1da",
   "metadata": {},
   "source": [
    "# Why would a single machine fail?\n",
    "- A single machine would fail due to limited disk space, memory constraints, and lack of fault tolerance. Hardware failure would result in data loss or downtime. Processing such a large dataset on one machine would also be extremely slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb54b8e",
   "metadata": {},
   "source": [
    "# Which Big Data technologies would become necessary?\n",
    "- At least three Big Data technologies would be required. HDFS would provide distributed and fault-tolerant storage. Apache Spark would enable fast, distributed data processing. Apache Hive would allow SQL-like querying on large datasets. Together, these tools support scalable and efficient Big Data analytics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
