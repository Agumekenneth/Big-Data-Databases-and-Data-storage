{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ebc605c",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302fea75",
   "metadata": {},
   "source": [
    "- pandas  for working with tables (DataFrames)\n",
    "- numpy  for generating random data efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3feeaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef972190",
   "metadata": {},
   "source": [
    "- Generating the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00399ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500_000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c71432a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a seed so that results are reproducible\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990ce0c",
   "metadata": {},
   "source": [
    "# Creating the dataset using a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f62e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "\n",
    "    # Unique ID for each transaction\n",
    "    \"transaction_id\": np.arange(1, N + 1),\n",
    "\n",
    "    # Generate random dates within one year\n",
    "    \"date\": pd.to_datetime(\"2023-01-01\") + pd.to_timedelta(\n",
    "        np.random.randint(0, 365, N), unit=\"D\"\n",
    "    ),\n",
    "\n",
    "    # Randomly assign regions\n",
    "    \"region\": np.random.choice(\n",
    "        [\"Central\", \"Eastern\", \"Western\", \"Northern\"], N\n",
    "    ),\n",
    "\n",
    "    # Random product categories\n",
    "    \"product_category\": np.random.choice(\n",
    "        [\"Food\", \"Electronics\", \"Clothing\", \"Furniture\"], N\n",
    "    ),\n",
    "\n",
    "    # Quantity of items bought (1 to 9)\n",
    "    \"quantity\": np.random.randint(1, 10, N),\n",
    "\n",
    "    # Unit price between 5 and 500\n",
    "    \"unit_price\": np.round(np.random.uniform(5, 500, N), 2),\n",
    "\n",
    "    # Payment methods\n",
    "    \"payment_method\": np.random.choice(\n",
    "        [\"Cash\", \"Mobile Money\", \"Card\"], N\n",
    "    ),\n",
    "\n",
    "    # Customer type\n",
    "    \"customer_type\": np.random.choice(\n",
    "        [\"Retail\", \"Wholesale\"], N\n",
    "    )\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e0591",
   "metadata": {},
   "source": [
    "- Creating a derived column for total transaction amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1bffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (23.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.12/bin/python3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acfb317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created successfully!\n"
     ]
    }
   ],
   "source": [
    "# total_amount = quantity * unit_price\n",
    "data[\"total_amount\"] = data[\"quantity\"] * data[\"unit_price\"]\n",
    "\n",
    "# Save the dataset as a CSV file (row-based storage)\n",
    "data.to_csv(\"transactions.csv\", index=False)\n",
    "\n",
    "# Save the dataset as a Parquet file (column-based storage)\n",
    "data.to_parquet(\"transactions.parquet\")\n",
    "\n",
    "print(\"Dataset created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ae0cc",
   "metadata": {},
   "source": [
    "# File size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f5ef31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file size: 32.86 MB\n",
      "Parquet file size: 7.03 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Get file size of CSV in megabytes\n",
    "csv_size_mb = os.path.getsize(\"transactions.csv\") / (1024 * 1024)\n",
    "\n",
    "# Get file size of Parquet in megabytes\n",
    "parquet_size_mb = os.path.getsize(\"transactions.parquet\") / (1024 * 1024)\n",
    "\n",
    "print(f\"CSV file size: {csv_size_mb:.2f} MB\")\n",
    "print(f\"Parquet file size: {parquet_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5f0b1",
   "metadata": {},
   "source": [
    "- The Parquet file is smaller than the CSV file because Parquet uses column-based storage and compression. CSV stores data as plain text, which is inefficient for large datasets. Big Data systems prefer columnar formats like Parquet because they reduce disk usage and improve query performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c14f6",
   "metadata": {},
   "source": [
    "# Read performance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33cf86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ba304a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV load time: 1.09 seconds\n"
     ]
    }
   ],
   "source": [
    "# Loading the entire CSV file\n",
    "start_time = time.time()\n",
    "df_csv = pd.read_csv(\"transactions.csv\")\n",
    "csv_load_time = time.time() - start_time\n",
    "print(f\"CSV load time: {csv_load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c998163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet load time: 1.51 seconds\n"
     ]
    }
   ],
   "source": [
    "#Loading the entire Parquet file\n",
    "start_time = time.time()\n",
    "df_parquet = pd.read_parquet(\"transactions.parquet\")\n",
    "parquet_load_time = time.time() - start_time\n",
    "print(f\"Parquet load time: {parquet_load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "addfcc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet (2 columns) load time: 0.09 seconds\n"
     ]
    }
   ],
   "source": [
    "#Loading only selected columns from Parquet\n",
    "start_time = time.time()\n",
    "df_parquet_cols = pd.read_parquet(\n",
    "    \"transactions.parquet\",\n",
    "    columns=[\"region\", \"total_amount\"]\n",
    ")\n",
    "parquet_cols_load_time = time.time() - start_time\n",
    "\n",
    "print(f\"Parquet (2 columns) load time: {parquet_cols_load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af53395",
   "metadata": {},
   "source": [
    "- Loading the Parquet file is faster than loading the CSV file because Parquet is optimized for analytical workloads. When only specific columns are selected, Parquet loads even faster and uses less memory. This demonstrates why Big Data systems avoid reading unnecessary data and instead scan only required columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da1fc80",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f49d36",
   "metadata": {},
   "source": [
    "- The CSV file size is: 32.86 MB\n",
    "- The Parquet file size is: 7.03 MB\n",
    "- The CSV file load time is: 1.09 seconds\n",
    "- The Parquet file load time: 1.51 seconds\n",
    "- The Parquet (2 columns) file load time is: 0.09 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c9464",
   "metadata": {},
   "source": [
    "The Parquet file was significantly smaller than the CSV file (7.03 MB vs 32.86 MB). This is because Parquet uses column-based storage and compression, while CSV stores data as plain text. Big Data systems prefer Parquet to reduce storage costs and improve I/O efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca44e9",
   "metadata": {},
   "source": [
    "Although the CSV file loaded slightly faster than the Parquet file, this is expected for moderate-sized datasets due to Parquetâ€™s decompression overhead. However, when only two columns were selected, Parquet loaded significantly faster. This demonstrates the advantage of columnar storage in analytical workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31513379",
   "metadata": {},
   "source": [
    "# Memory Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676998fd",
   "metadata": {},
   "source": [
    "- The memory used before was 90MB's loading the parquet file thus moderate\n",
    "- The memory used after by the RAM was 127MB's thus high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468fe4e2",
   "metadata": {},
   "source": [
    "- Memory usage increased from approximately 90 MB to 127 MB when loading the full dataset, indicating higher RAM consumption during full data scans. Column selection from Parquet reduced memory usage, highlighting why Big Data systems avoid reading unnecessary columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be061ac",
   "metadata": {},
   "source": [
    "# Big data problem on a small machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40315f",
   "metadata": {},
   "source": [
    "- Defining the number of rows to read at a time this is aimed at reducing memory usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce890e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 50_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a08428",
   "metadata": {},
   "source": [
    "- Creating the total number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02b69285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records: 500000\n"
     ]
    }
   ],
   "source": [
    "# Initializing a counter for total records\n",
    "total_records = 0\n",
    "\n",
    "# Reading the CSV file in chunks\n",
    "for chunk in pd.read_csv(\"transactions.csv\", chunksize=CHUNK_SIZE):\n",
    "    \n",
    "    # Adding the number of rows in the current chunk\n",
    "    total_records += len(chunk)\n",
    "print(\"Total number of records:\", total_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc4a65",
   "metadata": {},
   "source": [
    "- The total number of records was calculated by reading the CSV file in chunks instead of loading the entire dataset into memory. This approach prevents memory overload and is suitable for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f2c633",
   "metadata": {},
   "source": [
    "# Average Transaction Value per Product category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e477b456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average transaction value per category:\n",
      "{'Clothing': 1259.08098429613, 'Electronics': 1262.440670266557, 'Food': 1260.870070498439, 'Furniture': 1260.529748216433}\n"
     ]
    }
   ],
   "source": [
    "# Dictionaries to store cumulative sums and counts\n",
    "total_amount_sum = {}\n",
    "transaction_count = {}\n",
    "\n",
    "# Process the CSV file chunk by chunk\n",
    "for chunk in pd.read_csv(\"transactions.csv\", chunksize=CHUNK_SIZE):\n",
    "    \n",
    "    # Group data by product category within the chunk\n",
    "    grouped = chunk.groupby(\"product_category\")[\"total_amount\"].agg([\"sum\", \"count\"])\n",
    "    \n",
    "    # Accumulate results across all chunks\n",
    "    for category, row in grouped.iterrows():\n",
    "        total_amount_sum[category] = total_amount_sum.get(category, 0) + row[\"sum\"]\n",
    "        transaction_count[category] = transaction_count.get(category, 0) + row[\"count\"]\n",
    "\n",
    "# Calculate the average transaction value per category\n",
    "average_transaction_value = {\n",
    "    category: total_amount_sum[category] / transaction_count[category]\n",
    "    for category in total_amount_sum\n",
    "}\n",
    "\n",
    "print(\"Average transaction value per category:\")\n",
    "print(average_transaction_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
